{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54d74d0-92c6-4d0a-b8d0-2230df2db3a3",
   "metadata": {},
   "source": [
    "Q1. What is Elastic Net Regression and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684f934-2519-411b-a2a9-0008a023d422",
   "metadata": {},
   "source": [
    "Elastic Net Regression is a linear regression technique that combines two regularization methods: L1 regularization (Lasso) and L2 regularization (Ridge). It is designed to address some of the limitations of these two individual methods while taking advantage of their strengths.\n",
    "\n",
    "Elastic Net Regression differs from other regression techniques:\n",
    "\n",
    "1. Combination of L1 and L2 regularization:\n",
    "\n",
    "L1 regularization (Lasso) adds an absolute value penalty to the linear regression cost function, which encourages sparsity in the model by setting some coefficients to exactly zero. This helps with feature selection and can handle multicollinearity to some extent.\n",
    "\n",
    "L2 regularization (Ridge) adds a squared value penalty to the cost function, which discourages large coefficients and helps with multicollinearity by shrinking the coefficients toward zero.\n",
    "\n",
    "2. Control over sparsity:\n",
    "\n",
    "Elastic Net combines both L1 and L2 penalties in the regularization term. The regularization term is controlled by two hyperparameters: alpha and l1_ratio.\n",
    "The alpha parameter controls the overall strength of regularization, with a larger alpha resulting in stronger regularization. A value of alpha = 0 corresponds to linear regression (no regularization), and as alpha approaches infinity, all coefficients tend to zero.\n",
    "The l1_ratio parameter balances the contribution of L1 and L2 regularization. When l1_ratio = 0, Elastic Net becomes Ridge Regression, and when l1_ratio = 1, it becomes Lasso Regression. Intermediate values of l1_ratio allow you to combine both types of regularization.\n",
    "\n",
    "3. Handling multicollinearity:\n",
    "\n",
    "Elastic Net is effective at handling multicollinearity, a situation where predictor variables are highly correlated with each other. The combination of L1 and L2 regularization can select a subset of correlated variables and shrink the coefficients of the remaining variables.\n",
    "\n",
    "4. Variable selection:\n",
    "\n",
    "Lasso (L1 regularization) tends to produce sparse models by setting some coefficients to exactly zero. This makes it useful for feature selection, as it identifies the most important predictors.\n",
    "Ridge (L2 regularization) generally retains all features but shrinks their coefficients. It doesn't perform feature selection.\n",
    "Elastic Net combines the benefits of both L1 and L2 regularization, making it versatile for feature selection and coefficient shrinkage.\n",
    "\n",
    "5. Robustness:\n",
    "\n",
    "Elastic Net can be more robust than Lasso when there are many correlated predictors, as Lasso may arbitrarily select one from a group of highly correlated variables while Elastic Net can include them together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c407e-bf9f-462b-a891-46e4cc3bb4bd",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and disadvantages of Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b156f-ec8e-4db5-95d1-6afdc44730d6",
   "metadata": {},
   "source": [
    "Elastic Net Regression, as a regularization technique, comes with its own set of advantages and disadvantages, which make it suitable for certain situations and less suitable for others. Here are some of the main advantages and disadvantages of Elastic Net Regression:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Balanced Regularization: Elastic Net combines the strengths of L1 (Lasso) and L2 (Ridge) regularization, providing a balance between feature selection and coefficient shrinkage. This makes it more versatile compared to using Lasso or Ridge alone.\n",
    "\n",
    "Feature Selection: Elastic Net can perform automatic feature selection by setting some coefficients to exactly zero, effectively excluding irrelevant predictors from the model. This is particularly useful when dealing with high-dimensional data where feature selection is crucial.\n",
    "\n",
    "Multicollinearity Handling: Elastic Net is effective at handling multicollinearity by shrinking the coefficients of correlated variables, allowing them to be included in the model while reducing their impact.\n",
    "\n",
    "Robustness: Compared to Lasso, Elastic Net can be more stable and less prone to arbitrarily selecting one variable over another in cases of strong multicollinearity.\n",
    "\n",
    "Flexibility: The hyperparameters alpha and l1_ratio allow you to fine-tune the regularization strength and the balance between L1 and L2 penalties, giving you more control over the model's behavior.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Complexity: Elastic Net has two hyperparameters (alpha and l1_ratio) that need to be tuned, which can make model selection and hyperparameter tuning more complex than Ridge or Lasso regression.\n",
    "\n",
    "Computational Cost: Elastic Net can be computationally more expensive than ordinary linear regression, especially when dealing with a large number of predictors or data points.\n",
    "\n",
    "Less Intuitive: Interpreting the results of Elastic Net can be less intuitive than traditional linear regression since some coefficients may be exactly zero, while others are nonzero but shrunk.\n",
    "\n",
    "Loss of Information: While feature selection can be an advantage, it can also lead to a loss of potentially useful information if relevant features are mistakenly excluded.\n",
    "\n",
    "May Not Be Ideal for All Cases: Elastic Net is most beneficial when there is a mix of highly correlated predictors and you want to perform feature selection. However, for some specific problems, Ridge or Lasso regression alone may be more suitable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32932e60-8f4d-424b-941c-9cbd916a7b1b",
   "metadata": {},
   "source": [
    "Q4.  What are some common use cases for Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1961e-4c0c-4c92-9f18-e82001310a2c",
   "metadata": {},
   "source": [
    "Elastic Net Regression is a versatile regression technique that finds applications in various fields due to its ability to balance feature selection and coefficient shrinkage. \n",
    "\n",
    "Here are some common use cases for Elastic Net Regression:\n",
    "\n",
    "1. High-Dimensional Data Analysis: Elastic Net is particularly useful when dealing with datasets that have a large number of predictors (features) relative to the number of observations. It helps in automatic feature selection by setting some coefficients to zero, which is valuable for reducing overfitting.\n",
    "\n",
    "2. Multicollinearity Handling: When your dataset contains highly correlated predictors, Elastic Net can effectively address multicollinearity by shrinking the coefficients of correlated variables. This is common in fields like economics and finance, where many variables are interrelated.\n",
    "\n",
    "3. Biomedical Research: In fields such as genomics and proteomics, researchers often encounter high-dimensional datasets with many variables. Elastic Net can help identify relevant biomarkers or genes while controlling for the multicollinearity present in biological data.\n",
    "\n",
    "4. Finance: Predictive modeling in finance often involves dealing with large datasets of financial metrics. Elastic Net can be used to build models for stock price prediction, risk assessment, credit scoring, and portfolio optimization.\n",
    "\n",
    "5. Marketing and Customer Analytics: Elastic Net can be used to analyze customer data and predict customer behavior, such as churn prediction, customer lifetime value, or product recommendations. It helps in selecting the most influential features for marketing campaigns.\n",
    "\n",
    "6. Environmental Science: Environmental datasets often include a wide range of variables, some of which may be correlated. Elastic Net can help in modeling and predicting environmental phenomena like air quality, climate change, or species distribution.\n",
    "\n",
    "7. Image and Signal Processing: In image analysis and signal processing, where there are often many features or pixels to consider, Elastic Net can be used for feature selection and noise reduction. It's used in tasks such as image denoising, compression, and feature extraction.\n",
    "\n",
    "8. Text Analysis: Elastic Net can be applied to natural language processing (NLP) tasks like sentiment analysis, topic modeling, and document classification. It can help select relevant words or features while controlling for collinearity between words.\n",
    "\n",
    "9. Healthcare: In healthcare, Elastic Net can be used for disease prediction, patient outcome modeling, and identifying influential clinical factors while handling correlated variables and noisy data.\n",
    "\n",
    "10. Predictive Maintenance: In industries like manufacturing and aviation, Elastic Net can be applied to predict equipment failures or maintenance needs by analyzing sensor data and historical maintenance records.\n",
    "\n",
    "11. Social Sciences: Researchers in social sciences use Elastic Net for modeling and predicting social phenomena, such as educational outcomes, crime rates, or economic indicators.\n",
    "\n",
    "12. Quality Control: In manufacturing and quality control, Elastic Net can be used to identify key factors affecting product quality and process optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843f07e-0c97-437e-8b34-a7a352fa55f8",
   "metadata": {},
   "source": [
    "Q5. How do you interpret the coefficients in Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabce285-7c3f-4eee-b7b3-21ab495871bb",
   "metadata": {},
   "source": [
    "Interpreting the coefficients in Elastic Net Regression is somewhat more complex than in ordinary linear regression due to the combined effects of L1 (Lasso) and L2 (Ridge) regularization. The interpretation depends on whether a coefficient is zero, positive, or negative, as well as its magnitude. \n",
    "\n",
    "Here's how you can interpret the coefficients:\n",
    "\n",
    "1. Nonzero Coefficients: Nonzero coefficients in Elastic Net indicate the strength and direction of the relationship between a predictor variable and the target variable. A positive coefficient means that an increase in the predictor's value leads to an increase in the target variable's value, while a negative coefficient means the opposite. The magnitude of the coefficient reflects the strength of this relationship.\n",
    "\n",
    "2. Zero Coefficients: Coefficients that are exactly zero in Elastic Net indicate that the corresponding predictor variable has been excluded from the model. This implies that the variable is considered irrelevant or redundant for predicting the target variable.\n",
    "\n",
    "3. Magnitude of Coefficients: The magnitude of nonzero coefficients can be compared to assess the relative importance of different predictors. Larger magnitudes indicate stronger effects, while smaller magnitudes indicate weaker effects. Keep in mind that the magnitude of coefficients can be influenced by the regularization strength (controlled by the alpha hyperparameter). Higher alpha values lead to smaller coefficients.\n",
    "\n",
    "4. L1 and L2 Effects: Elastic Net combines L1 and L2 regularization, which can have different effects on coefficients. The L1 component encourages sparsity by pushing some coefficients to zero (similar to Lasso), while the L2 component shrinks all coefficients towards zero. The relative importance of L1 and L2 regularization is controlled by the l1_ratio hyperparameter. A l1_ratio of 0 corresponds to pure L2 regularization (similar to Ridge), while a l1_ratio of 1 corresponds to pure L1 regularization (similar to Lasso). Intermediate values of l1_ratio combine the two effects.\n",
    "\n",
    "5. Significance Testing: When interpreting coefficients, it's essential to consider their statistical significance. You can perform hypothesis tests (e.g., t-tests) to determine if a coefficient is significantly different from zero. This helps you assess whether a predictor is genuinely contributing to the model's predictive power.\n",
    "\n",
    "6. Interaction Effects: Elastic Net coefficients can also represent interaction effects between predictors. If you have interaction terms in your model, the coefficients associated with those terms indicate how the interaction influences the target variable.\n",
    "\n",
    "7. Scaling Effects: The scale of predictor variables can affect the magnitude of coefficients. If your predictors are on different scales, the corresponding coefficients will be on different scales as well. Standardizing or scaling the predictors can make it easier to compare the importance of different variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbaf3d3-c0c1-44df-a866-03633d6eaa2a",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values when using Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242a6c1-4997-473a-a201-f0775306d33d",
   "metadata": {},
   "source": [
    "Handling missing values is an important step when using Elastic Net Regression, or any regression technique for that matter. Missing data can lead to biased or unreliable model results. Here are several strategies to handle missing values when applying Elastic Net Regression:\n",
    "\n",
    "1. Remove Rows with Missing Values:\n",
    "\n",
    "The simplest approach is to remove rows (samples) that contain missing values. This is appropriate when the amount of missing data is small and won't significantly impact the analysis. However, it can lead to a loss of valuable data if many rows have missing values.\n",
    "\n",
    "2. Imputation with Mean, Median, or Mode:\n",
    "\n",
    "You can replace missing values in a particular column with the mean, median, or mode value of that column. This is a straightforward method and can be effective if the missing values are missing at random and the data follows a normal distribution. Use the median if your data has outliers that might skew the mean.\n",
    "\n",
    "3. Imputation with a Specific Value:\n",
    "\n",
    "Sometimes, it's more meaningful to impute missing values with a domain-specific constant. For example, you might replace missing values in a survey where \"missing\" means \"did not respond\" with a value like -1 or 0 to distinguish them from other responses.\n",
    "\n",
    "4. Imputation with Predictive Modeling:\n",
    "\n",
    "You can use other variables that are not missing to predict the missing values in a column. This approach involves creating a regression model to predict the missing values based on the values of other variables. Techniques like linear regression or k-nearest neighbors can be used for this purpose.\n",
    "\n",
    "5. Interpolation and Extrapolation:\n",
    "\n",
    "For time series data, you can use interpolation techniques (e.g., linear interpolation) to estimate missing values based on the values of adjacent time points. Extrapolation can be used to estimate missing values outside the observed time range.\n",
    "\n",
    "6. Multiple Imputation:\n",
    "\n",
    "Multiple Imputation is a more advanced technique where multiple imputed datasets are created, each with different imputed values, to account for uncertainty in imputation. You run the regression model on each imputed dataset separately and then combine the results to get more accurate parameter estimates and standard errors.\n",
    "\n",
    "7. Feature Engineering:\n",
    "\n",
    "Create additional binary indicator variables that flag whether a value is missing or not for each column with missing data. This can help the model learn from the patterns of missingness.\n",
    "Domain Knowledge:\n",
    "\n",
    "If you have domain knowledge about the data and the reasons for missing values, you can make informed decisions on how to handle them. For example, you may decide to treat missing values differently based on their cause or significance in the context of your analysis.\n",
    "Use Algorithms that Handle Missing Values:\n",
    "\n",
    "Some machine learning algorithms, including certain implementations of Elastic Net, can handle missing values directly. They do this by adapting the model fitting process to work with incomplete data. Check the documentation of the specific library or tool you're using to see if it supports this feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ae552-111f-4c22-877d-912b8659af6d",
   "metadata": {},
   "source": [
    "Q7. How do you use Elastic Net Regression for feature selection? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fed7fd-6ef4-4c30-b3a8-122181d1a229",
   "metadata": {},
   "source": [
    "Elastic Net Regression is a powerful technique for feature selection because it combines L1 (Lasso) regularization, which encourages sparsity by setting some coefficients to exactly zero, with L2 (Ridge) regularization. This allows you to effectively select a subset of the most important features while handling multicollinearity and controlling overfitting. Here's how you can use Elastic Net Regression for feature selection:\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "To prepare your dataset by handling missing values and encoding categorical variables if necessary.\n",
    "Standardize or scale your predictor variables to ensure that they have similar magnitudes. This step can help in interpreting the importance of coefficients.\n",
    "\n",
    "Choose Appropriate Hyperparameters:\n",
    "\n",
    "Elastic Net has two main hyperparameters: alpha and l1_ratio.\n",
    "Adjust the alpha hyperparameter to control the overall strength of regularization. Higher values of alpha will result in stronger regularization, which is more likely to lead to feature selection. You can perform cross-validation to find the optimal alpha value.\n",
    "Adjust the l1_ratio hyperparameter to control the balance between L1 and L2 regularization. A l1_ratio of 1 corresponds to pure L1 regularization (Lasso), which is more likely to lead to feature selection. A l1_ratio of 0 corresponds to pure L2 regularization (Ridge).\n",
    "Experiment with different combinations of alpha and l1_ratio to achieve the desired level of sparsity.\n",
    "\n",
    "Fit Elastic Net Model:\n",
    "\n",
    "Train an Elastic Net Regression model on your dataset using the chosen hyperparameters.\n",
    "\n",
    "Examine Coefficient Values:\n",
    "\n",
    "Examine the coefficients of the predictor variables.\n",
    "Coefficients that are exactly zero indicate that the corresponding features have been selected out of the model. These are the features that Elastic Net has determined to be the least important for predicting the target variable.\n",
    "\n",
    "Rank and Select Features:\n",
    "\n",
    "Rank the remaining features based on the magnitude of their coefficients. Larger absolute values of coefficients typically indicate more important features.\n",
    "You can use various criteria to decide which features to keep, such as selecting the top N features with the largest coefficients or setting a threshold for the coefficient magnitude.\n",
    "Another common approach is to use recursive feature elimination (RFE) or sequential feature selection algorithms to iteratively select the most important features based on model performance.\n",
    "\n",
    "Evaluate Model Performance:\n",
    "\n",
    "After selecting the subset of features, retrain your Elastic Net model using only those features and evaluate its performance on a validation or test dataset.\n",
    "Assess the model's performance metrics, such as mean squared error (MSE), R-squared, or others, to ensure that the reduced feature set provides a good balance between model complexity and predictive accuracy.\n",
    "\n",
    "Iterate if Necessary:\n",
    "\n",
    "If the initial feature selection does not yield satisfactory results, you can iterate and experiment with different hyperparameters or feature selection criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518a11e-e6e0-4422-bdba-24e463f27c51",
   "metadata": {},
   "source": [
    "Q8. How do you pickle and unpickle a trained Elastic Net Regression model in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515c3dc-de4d-4d95-b4d1-0184f1897652",
   "metadata": {},
   "source": [
    "In Python, we can use the pickle module to serialize (pickle) a trained Elastic Net Regression model and save it to a file, and later you can unpickle (load) the model from the file for reuse. \n",
    "\n",
    "Pickle (Serialize) a Trained Elastic Net Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1eda81-4d54-4992-8661-92bb1e26dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = {\n",
    "    'name': 'John Doe',\n",
    "    'age': 30,\n",
    "    'city': 'New York',\n",
    "    'interests': ['programming', 'hiking', 'reading'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55718302-e7bb-4272-ae7b-651fc13a1f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'John Doe',\n",
       " 'age': 30,\n",
       " 'city': 'New York',\n",
       " 'interests': ['programming', 'hiking', 'reading']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99d323f7-ad2f-43c0-9efe-102cc0d9b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'John Doe', 'age': 30, 'city': 'New York', 'interests': ['programming', 'hiking', 'reading']}\n"
     ]
    }
   ],
   "source": [
    "with open('sample_dataset.pkl', 'wb') as data_file:\n",
    "    pickle.dump(data, data_file)\n",
    "    \n",
    "    \n",
    "with open('sample_dataset.pkl', 'rb') as data_file:\n",
    "    loaded_data = pickle.load(data_file)\n",
    "\n",
    "print(loaded_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db367f82-5e2d-4d62-b406-fe9961933474",
   "metadata": {},
   "source": [
    "Unpickle (Deserialize) a Trained Elastic Net Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04e6d7fc-5617-46f6-b28e-2b3eabbdc6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: John Doe\n",
      "Age: 30\n",
      "City: New York\n",
      "Interests: ['programming', 'hiking', 'reading']\n"
     ]
    }
   ],
   "source": [
    "with open('sample_dataset.pkl', 'rb') as data_file:\n",
    "    loaded_data = pickle.load(data_file)\n",
    "\n",
    "# Access the loaded_data dictionary\n",
    "print(\"Name:\", loaded_data['name'])\n",
    "print(\"Age:\", loaded_data['age'])\n",
    "print(\"City:\", loaded_data['city'])\n",
    "print(\"Interests:\", loaded_data['interests'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea87b0-6f6f-4d2d-abab-8464bc3515da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937bb51-6799-4607-b5e9-36e9e392b4dc",
   "metadata": {},
   "source": [
    "\n",
    "In machine learning, the purpose of pickling (serializing) a model is to save the trained model and its associated parameters to a file so that it can be easily reused or deployed in the future without the need to retrain it. Pickling allows you to save the state of a trained model, including its architecture, weights, and hyperparameters, in a compact and portable format. Here are several key reasons why pickling a model is important:\n",
    "\n",
    "    Reusability: Pickled models can be reused in different environments and scenarios. You can load a trained model and make predictions on new data without the need to retrain it, saving both time and computational resources.\n",
    "\n",
    "    Deployment: Pickling is often used when deploying machine learning models in production systems. You can save a trained model and load it within your production environment to provide real-time predictions or recommendations.\n",
    "\n",
    "    Sharing and Collaboration: Pickled models can be easily shared with colleagues or collaborators, allowing them to reproduce your results or integrate the model into their own work. This is especially useful for open-source projects or collaborative research.\n",
    "\n",
    "    Version Control: Saving trained models as pickle files can be part of version control systems (e.g., Git) to keep track of model versions and facilitate collaboration among data scientists and engineers.\n",
    "\n",
    "    Experimentation: When conducting experiments with different model architectures, hyperparameters, or datasets, pickling models allows you to save and compare the performance of various model configurations efficiently.\n",
    "\n",
    "    Offline Analysis: You can save trained models for later analysis, debugging, or interpretation. This can be helpful for understanding how a model makes predictions or diagnosing issues.\n",
    "\n",
    "    Model Ensembles: Pickling enables the creation of model ensembles, where multiple trained models are saved and combined to make predictions, often resulting in improved performance compared to individual models.\n",
    "\n",
    "    Scalability: Pickling is particularly useful when you have trained a complex model that requires significant computational resources. By saving the model, you can deploy it on a scalable infrastructure or cloud platform as needed.\n",
    "\n",
    "    Protection Against Code Changes: Storing a pickled model allows you to ensure that the model remains consistent even if the codebase or libraries change over time. This is important for maintaining the integrity of model results.\n",
    "\n",
    "Batch Processing: For scenarios where you need to apply a model to large batches of data, you can load a pickled model and make predictions in a batch-processing mode efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27597d3-2628-47a8-b484-b2af0711e01a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
